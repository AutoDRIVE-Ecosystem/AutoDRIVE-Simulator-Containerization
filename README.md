# AutoDRIVE Simulator HPC

This repository contains HPC deployments of the AutoDRIVE Simulator.
todo: add overall project description

# SETUP

**Pre-requisites**: Kubectl, Docker, Python 3.8+, python packages listed in requirements.txt, download desired versions
of AutoDRIVE simulator & AutoDRIVE devkit, configure Kubectl with access to desired cluster.
<!-- todo: add setup steps
- install k8
- install docker 
- install python & necessary packages
 - requirement.txt
- Download AutoDRIVE Devkit & Simulator
- Pull Dockerfiles -->

# USAGE

If using Clemson University's Rancher cluster, ```example_script.py``` is a script which provides a baseline use case.

## File Structure

**Docker/**: The ```Docker``` directory contains all necessary files to compile docker images containing the AutoDRIVE
Simulator, AutoDRIVE Devkit, simulation webviewer, and the backend control server. A Makefile contains necessary commands
to compile each docker image. The dockerfiles expect ```AutoDRIVE_API``` and ```AutoDRIVE_Simulator``` directories to be
placed here (populated with simulator & devkit files). 

It should be noted the ```logger.py``` file may need to be moved into a new AutoDRIVE_API folder & integrated into the
AutoDRVIE vehicle script in order to enable data collection from pods inside the cluster.

**Kubernetes/**: The ```Kubernetes``` directory contains the yaml files for deployments used in the cluster. 

**Python/**: The ```Python``` directory holds all of the scripts to control simulations running in the cluster, namely
```automation_module.py```. Most variables can be updated using the ```config.ini``` file. 

## Known Issues

1. The control server can sometimes crash if simulation conditions are not configured to have multiple conditions to iterate
through. This can generally be fixed by restarting the pod or adding duplicate conditions based on the desired number of
iterations. A current goal of this project is to re-implement the way simulation configurations are handled to not be
generated by the control server/inside the cluster.

2. Sometimes queries to the kubernetes api inside the cluster can result in network errors (both if called via python
subprocess + kubetl or via the python kubernetes client). There is a large amount of timeouts/repeat attempts built into
the automation library, but it can still be an issue for large workloads querying the same node in a cluster multiple 
times.

**NOTE: this readme is *still being updated!* Some details of the setup & usage are currently missing.**
